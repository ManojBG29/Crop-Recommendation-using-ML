# -*- coding: utf-8 -*-
"""Crop_Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lCT5Yhe_7_-lRMtxG3tV5zGHrAJ_jqnq

# **Crop Recommendation using ML**
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the neccessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Import and Load the dataset
data= pd.read_csv('/content/Crop_recommendation.csv')

# Top 3 rows of the dataset
data.head(3)

# Random 3 rows of the dataset
data.sample(3)

# Displays the number of rows and columns in the dataset
data.shape

# Check for the presence of any null values if present
data.isnull().sum()

# General information on the dataset
data.info()

# Assigning x and y their respective required columns
x= data.drop(['label'], axis= 'columns')
y= data['label']

x[:5]

y[:5]

# unique number of each type of crop
y.value_counts()

# Fit the scaler on the data and transform the features
scaler = MinMaxScaler()     # MinMax Scaler ensures all the values lie between 0 and 1 so that it would be easier for the machine to learn
x_scaled = scaler.fit_transform(x)

x_scaled[:4]

# LabelEncoder is used for transforming the categorical variables for further processing
le= LabelEncoder()  # It ensures that each crop is given a numerical value so that machine learning can be carried out.
y = le.fit_transform(y)

y[:7]

# Entire dataset divided into training and testing dataset
x_train, x_test, y_train, y_test= train_test_split(x_scaled,y, test_size= 0.2)

len(x_train), len(x_test)

"""Grid Search is followed in the following cases to know the best combination of parameters of respective machine learning algorithms and obtain maximum accuracy in classification. Also the crops were inverse transformed to know the it by string type rather than numerical type."""

# Define the hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}
# Fit the grid search to the data
svc = SVC()
grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)

# Print the best parameters and their corresponding accuracy
print("Best Parameters: ", grid_search.best_params_)
print("Best Accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

# Make predictions using the best model
best_svc = grid_search.best_estimator_
y_pred = best_svc.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy using Best Model: {:.2f}%".format(accuracy * 100))

# If you need to inverse transform the encoded labels back to the original labels
decoded_crops = le.inverse_transform(y)
print("Decoded Crops:", decoded_crops)

# Suppose to predict the crop for a given conditions and availabilities
print(f'The suitable crop is {decoded_crops[best_svc.predict([[90,42,43,20,82,6.5,202.3]])]}')

# Define the hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # 1 for Manhattan distance (L1), 2 for Euclidean distance (L2)
}

# Fit the grid search to the data
knn = KNeighborsClassifier()
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)

# Print the best parameters and their corresponding accuracy
print("Best Parameters: ", grid_search.best_params_)
print("Best Accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

# Make predictions using the best model
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy using Best Model: {:.2f}%".format(accuracy * 100))

# If you need to inverse transform the encoded labels back to the original labels
decoded_crops = le.inverse_transform(y)
print("Decoded Labels:", decoded_crops)

# Suppose to predict the crop for a given conditions and availabilities
print(f'The suitable crop is {decoded_crops[best_knn.predict([[90,42,43,20,82,6.5,202.3]])]}')

# Create a DecisionTreeClassifier
dt = DecisionTreeClassifier()

# Define the hyperparameter grid
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Fit the grid search to the data
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)

# Print the best parameters and their corresponding accuracy
print("Best Parameters: ", grid_search.best_params_)
print("Best Accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

# Make predictions using the best model
best_dt_clf = grid_search.best_estimator_
y_pred = best_dt_clf.predict(x_test)

# Evaluate the accuracy on the test set
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy using Best Model: {:.2f}%".format(accuracy * 100))

# If you need to inverse transform the encoded labels back to the original labels
decoded_crops = le.inverse_transform(y)
print("Decoded crops:", decoded_crops)

# Suppose to predict the crop for a given conditions and availabilities
print(f'The suitable crop is {decoded_crops[best_dt_clf.predict([[90,42,43,20,82,6.5,202.3]])]}')

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [ 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [ 'sqrt', 'log2']
}

# Create a Random Forest classifier
rf = RandomForestClassifier()

# Initialize GridSearchCV
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(x_train, y_train)

# Print the best parameters and their corresponding accuracy
print("Best Parameters: ", grid_search.best_params_)
print("Best Accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

# Make predictions using the best model
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy using Best Model: {:.2f}%".format(accuracy * 100))

# If you need to inverse transform the encoded labels back to the original labels
decoded_crops = le.inverse_transform(y)
print("Decoded Labels:", decoded_crops)

# Suppose to predict the crop for a given conditions and availabilities
print(f'The suitable crop is {decoded_crops[best_rf.predict([[90,42,43,20,82,6.5,202.3]])]}')

"""In conclusion, the classification task using Support Vector Classifier (SVC), k-Nearest Neighbors (KNN), Decision Tree (DT) and Random Forest (RF) demonstrated promising results. All the algorithms tend to work well both on training and testing data with higher classification scores due to proper Cross-Validation and Grid Search Techniques. Grid Search particularly for tuning the hyperparameters to decrease the cost function and improve the performance of the model. Data was preprocessed using Min Max scalers and categorical variables were encoded. Future line of work includes its extension to other algorithms and including weather parameters as well."""